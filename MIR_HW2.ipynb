{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmdrez4/NLP_bio_data_analysis/blob/main/MIR_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <strong><font color=#A52A2A></span> MIR_HW3 </strong>\n",
        "\n",
        "98101566 -> Mohammadreza Daviran\n",
        "<br>\n",
        "98105919 -> Reza Erfan Arani\n",
        "<br>\n",
        "98106434 -> Mohammad Khodadadi Aski\n",
        "<br>\n",
        "\n",
        "<strong><font color=#B22222></span> Please run this notebook on google colab! </strong>"
      ],
      "metadata": {
        "id": "Dul1eni14UhG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTVGDb3hIVmX"
      },
      "source": [
        "# <strong><font color=#5F9EA0></span> Download the files </strong> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2x1_NB3_NOVl"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH7qogTpOdBg",
        "outputId": "152e5644-644a-44d6-daa5-3b474c1d3142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'course-nlp-ir-1-text-exploring' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/language-ml/course-nlp-ir-1-text-exploring.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXIGBR5SOgjd",
        "outputId": "87b40ecc-7fcb-4d65-8b2b-98972c47ee59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/course-nlp-ir-1-text-exploring/exploring-datasets/health\n"
          ]
        }
      ],
      "source": [
        "%cd course-nlp-ir-1-text-exploring/exploring-datasets/health"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sv7vDB6ZO1ip",
        "outputId": "af16dd2d-0209-426b-fb07-7fae7d1f82aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bio.json\t hidoctor-3.json  namnak-1.json  namnak-4.json\tstopwords.txt\n",
            "hidoctor-1.json  hidoctor-4.json  namnak-2.json  namnak-5.json\n",
            "hidoctor-2.json  hidoctor-5.json  namnak-3.json  README.md\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternative way to download the files:"
      ],
      "metadata": {
        "id": "yhuA9EIbpf4C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yJGTfwvFPie"
      },
      "outputs": [],
      "source": [
        "# files = []\n",
        "# result = list()\n",
        "# for i in range(1, 6):\n",
        "#   link = \"https://github.com/language-ml/course-nlp-ir-1-text-exploring/blob/main/exploring-datasets/health/hidoctor-\" + str(i) + \".json\"\n",
        "#   hidoctor = requests.get(link, allow_redirects=True)\n",
        "#   name = 'hidoctor-' + str(i) + '.json'\n",
        "#   temp_file = open(name, 'wb').write(hidoctor.content)\n",
        "#   files.append(name)\n",
        "\n",
        "\n",
        "# for i in range(1, 6):\n",
        "#   link = \"https://github.com/language-ml/course-nlp-ir-1-text-exploring/blob/main/exploring-datasets/health/namnak-\" + str(i) + \".json\"\n",
        "#   namnak = requests.get(link, allow_redirects=True)\n",
        "#   name = 'namnak-' + str(i) + '.json'\n",
        "#   temp_file = open(name, 'wb').write(namnak.content)\n",
        "#   files.append(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset is json so first of all we merge all the files to 'bio.json' file."
      ],
      "metadata": {
        "id": "d2NrMF9_pklF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTYq4-VI7WW9"
      },
      "outputs": [],
      "source": [
        "def merge_JsonFiles(filename):\n",
        "    result = list()\n",
        "    for f1 in filename:\n",
        "        with open(f1, 'r') as infile:\n",
        "            result.extend(json.load(infile))\n",
        "\n",
        "    with open('bio.json', 'w') as output_file:\n",
        "        json.dump(result, output_file)\n",
        "\n",
        "files = ['hidoctor-1.json', 'hidoctor-2.json', 'hidoctor-3.json','hidoctor-4.json','hidoctor-5.json', 'namnak-1.json', 'namnak-2.json', 'namnak-3.json','namnak-4.json','namnak-5.json']\n",
        "merge_JsonFiles(files)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By loading the json file, we extract all the paragraphs and save them in *documents* for future work."
      ],
      "metadata": {
        "id": "xeRQB1m1pwB9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qsxSn8lCfMD"
      },
      "outputs": [],
      "source": [
        "f = open('bio.json')\n",
        "bioset = json.load(f)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K604XMuY39K"
      },
      "outputs": [],
      "source": [
        "documents = []\n",
        "for i in bioset:\n",
        "  documents.append(i['paragraphs'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZWE3sU4JLjY"
      },
      "source": [
        "# <strong><font color=#5F9EA0></span> Preprocessing </strong> "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we import the requirements for preprocessing the data."
      ],
      "metadata": {
        "id": "tuRFsfnEqqOk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnMeJTB7QQcL"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "import tqdm\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk import word_tokenize\n",
        "import csv\n",
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import random \n",
        "import numpy as np\n",
        "import nltk\n",
        "import codecs\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEp1MQ0PVKjd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af56abec-a408-49dc-ec09-c2056f7bebd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: hazm in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.7/dist-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: libwapiti>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tX9SSCH-Kqpc"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlOB6aLdaxLf"
      },
      "source": [
        "**normalize**\n",
        "\n",
        "normalizer function normilizes the docs we extracted and saves them \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCDBAkhV1z7N"
      },
      "outputs": [],
      "source": [
        "normalizer = Normalizer()\n",
        "\n",
        "doc_normalized = [[normalizer.normalize(y) for y in x] for x in tqdm.tqdm(documents)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4F3QHC8UhbPY"
      },
      "outputs": [],
      "source": [
        "doc_normalized_edited = []\n",
        "for doc in doc_normalized:\n",
        "  temp = []\n",
        "  for x in doc:\n",
        "    temp.append(x.replace('\\u200c', ' '))\n",
        "  doc_normalized_edited.append(temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krf6gcD-13EE"
      },
      "source": [
        "**wording**\n",
        "\n",
        "here we tokenize the sentences in each normilized document and then we save them in the doc_sentences array "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGhgVtpT1nFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26a163dc-e4d5-47ab-8fc2-6d0de7f2dc2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2125/2125 [00:00<00:00, 6184.95it/s]\n"
          ]
        }
      ],
      "source": [
        "doc_sentences = [sent_tokenize(' '.join(x)) for x in tqdm.tqdm(doc_normalized_edited)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTea4fdk17ZQ"
      },
      "source": [
        "**tokenize**\n",
        "\n",
        "we tokenize the words in the doc_sentences array and the we save them into doc_tokens array by tqdm tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ah83qkI1vjg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a49c8ef7-80bd-4675-fe2a-091c16f32b46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2125/2125 [00:02<00:00, 869.13it/s]\n"
          ]
        }
      ],
      "source": [
        "doc_tokens = [[word_tokenize(sent) for sent in sents] for sents in tqdm.tqdm(doc_sentences)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LCwvfkRb7Hy"
      },
      "source": [
        "Then by using itertools save them in all_tokens. Because we want to exclude the stopwords from our tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtPXv8ZRIuQx"
      },
      "outputs": [],
      "source": [
        "all_tokens = []\n",
        "for i in doc_tokens:\n",
        "  all_tokens.append(list(itertools.chain.from_iterable(i)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAcWcUdIH86r"
      },
      "source": [
        "**Delete stop words**\n",
        "\n",
        "here we \n",
        "\n",
        "\n",
        "1.   download the persian stopwords from bellow link\n",
        "2.   then we delete them from all_tokens arraylist cause they have no value for us\n",
        "3. save the tokens which are not stopwords in all_tokens_nonstop arraylist\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fBlxGsBA0bH"
      },
      "outputs": [],
      "source": [
        "import codecs\n",
        "link = \"https://raw.githubusercontent.com/sobhe/hazm/master/hazm/data/stopwords.dat\"\n",
        "temp_file = open('stopwords.txt', 'wb').write(requests.get(link, allow_redirects=True).content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "getNj9JSA-7z"
      },
      "outputs": [],
      "source": [
        "stopwords = [normalizer.normalize(x.strip()) for x in codecs.open('stopwords.txt','r','utf-8').readlines()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udEmVGOGI3c-"
      },
      "outputs": [],
      "source": [
        "all_tokens_nonstop = []\n",
        "for doc in all_tokens:\n",
        "  all_tokens_nonstop.append([t for t in tqdm.tqdm(doc) if t not in stopwords])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jQZmE4hMhXr"
      },
      "source": [
        "**lemmatization, stemming**\n",
        "\n",
        "here we lemmatize and stem the tokens "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATzrOSutMijX"
      },
      "outputs": [],
      "source": [
        "stemmer = Stemmer()\n",
        "lemmatizer = Lemmatizer()\n",
        "\n",
        "def get_lemma_set(tok, opt=1):\n",
        "    if opt == 1:\n",
        "        return stemmer.stem(tok)\n",
        "    if opt == 2:\n",
        "        return lemmatizer.lemmatize(tok)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dst1LATNMvRu"
      },
      "outputs": [],
      "source": [
        "opt = 2\n",
        "\n",
        "all_tokens_nonstop_lemstem = []\n",
        "\n",
        "for doc in all_tokens_nonstop:\n",
        "  all_tokens_nonstop_lemstem.append([get_lemma_set(t, opt) for t in tqdm.tqdm(doc)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVlyHY3kKrhK"
      },
      "source": [
        "# <strong><font color=#5F9EA0></span> Boolean retrieval </strong> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgIHcLbxFuYo"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize , word_tokenize\n",
        "import glob\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqZvCNbNjxec"
      },
      "source": [
        "all words longer than 2 characters are stored in bag_of_words array \n",
        "all unique words in the bag of words set are stored in unique_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFjYDWijHRgR"
      },
      "outputs": [],
      "source": [
        "bag_of_words = []\n",
        "unique_words = set()\n",
        "lemmatized_docs = [' '.join(x) for x in all_tokens_nonstop_lemstem]\n",
        "for doc in lemmatized_docs:\n",
        "  temp = doc.split(' ')\n",
        "  bag_of_word = [t for t in temp if len(t) > 2]\n",
        "  bag_of_words.append(bag_of_word)\n",
        "  unique_words = set(unique_words).union(set(bag_of_word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBouGczZkKhA"
      },
      "source": [
        "we specify which words from unique_words each document contains and store their number in num_of_words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laZZoggBJyIe"
      },
      "outputs": [],
      "source": [
        "num_of_words = []\n",
        "\n",
        "for i in range(len(bag_of_words)):\n",
        "  numOfWord = dict.fromkeys(unique_words, 0)\n",
        "  for word in bag_of_words[i]:\n",
        "    numOfWord[word] += 1\n",
        "  num_of_words.append(numOfWord)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We illustrate the num_of_words list for the first 10 documents for better comprehension:"
      ],
      "metadata": {
        "id": "_ruonMSWtDzF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "7StRxbDuMLBn",
        "outputId": "09a75d29-7e86-41fa-cf9a-6bf1a18702cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   اشتهای  عفونی  اکسون  _گرسنگی  صیه  خواهند_مصرف  شادکامی  غذاخورده  Jose  \\\n",
              "0       0      0      0        0    0            0        0         0     0   \n",
              "1       0      0      0        0    0            0        0         0     0   \n",
              "2       0      0      0        0    0            0        0         0     0   \n",
              "3       0      0      0        0    0            0        0         0     0   \n",
              "4       0      0      0        0    0            0        0         0     0   \n",
              "5       0      0      0        0    0            0        0         0     0   \n",
              "6       0      2      0        0    0            0        0         0     0   \n",
              "7       0      0      0        0    0            0        0         0     0   \n",
              "8       0      0      0        0    0            0        0         0     0   \n",
              "9       0      0      0        0    0            0        0         0     0   \n",
              "\n",
              "   کاهو  ...  دیمتیل  بیوتکنولوژیک  اوروفارنکس  وافل  کلانژیت  بیهوشی  آلکسی  \\\n",
              "0     0  ...       0             0           0     0        0       0      0   \n",
              "1     0  ...       0             0           0     0        0       0      0   \n",
              "2     0  ...       0             0           0     0        0       0      0   \n",
              "3     0  ...       0             0           0     0        0       0      0   \n",
              "4     0  ...       0             0           0     0        0       0      0   \n",
              "5     0  ...       0             0           0     0        0       0      0   \n",
              "6     0  ...       0             0           0     0        0       0      0   \n",
              "7     0  ...       0             0           0     0        0       0      0   \n",
              "8     0  ...       0             0           0     0        0       0      0   \n",
              "9     0  ...       0             0           0     0        0       0      0   \n",
              "\n",
              "   Konsyl  بولدو  B۲و  \n",
              "0       0      0    0  \n",
              "1       0      0    0  \n",
              "2       0      0    0  \n",
              "3       0      0    0  \n",
              "4       0      0    0  \n",
              "5       0      0    0  \n",
              "6       0      0    0  \n",
              "7       0      0    0  \n",
              "8       0      0    0  \n",
              "9       0      0    0  \n",
              "\n",
              "[10 rows x 26575 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-566f7b0b-992b-440c-9156-86f10513b89e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>اشتهای</th>\n",
              "      <th>عفونی</th>\n",
              "      <th>اکسون</th>\n",
              "      <th>_گرسنگی</th>\n",
              "      <th>صیه</th>\n",
              "      <th>خواهند_مصرف</th>\n",
              "      <th>شادکامی</th>\n",
              "      <th>غذاخورده</th>\n",
              "      <th>Jose</th>\n",
              "      <th>کاهو</th>\n",
              "      <th>...</th>\n",
              "      <th>دیمتیل</th>\n",
              "      <th>بیوتکنولوژیک</th>\n",
              "      <th>اوروفارنکس</th>\n",
              "      <th>وافل</th>\n",
              "      <th>کلانژیت</th>\n",
              "      <th>بیهوشی</th>\n",
              "      <th>آلکسی</th>\n",
              "      <th>Konsyl</th>\n",
              "      <th>بولدو</th>\n",
              "      <th>B۲و</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 26575 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-566f7b0b-992b-440c-9156-86f10513b89e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-566f7b0b-992b-440c-9156-86f10513b89e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-566f7b0b-992b-440c-9156-86f10513b89e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "df = pd.DataFrame(num_of_words[0:10])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we normalize and lemmatize our query."
      ],
      "metadata": {
        "id": "L6WMgChqt0xZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhkYEGewEUc4"
      },
      "outputs": [],
      "source": [
        "input = 'علل ضعف بینایی'\n",
        "splitted_input = input.split(\" \")\n",
        "nsi = []\n",
        "k = 12\n",
        "for x in splitted_input:\n",
        "  if (x not in stopwords and len(x) > 2 ):\n",
        "    nsi.append(normalizer.normalize(lemmatizer.lemmatize(x))) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare the query with our documents, we just need to find out in which documents the words of our query are. So we assign score for each doc with respect to the existence of words and the number of them. \n",
        "\n",
        "<strong><font color=#cf6679></span> We consider the number of words, so our implemented method is better than the ordinary Boolean method. If we want to implement ordinary boolean method we simply can comment one line!</strong> \n",
        "\n",
        "if you want to make the algorithm better , you can assume a negative point for not having a word"
      ],
      "metadata": {
        "id": "oSxzF1uDuDtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score = {}\n",
        "for i in range(len(all_tokens_nonstop_lemstem)):\n",
        "  score[i] = 0\n",
        "\n",
        "counter = 0\n",
        "for doc in num_of_words:\n",
        "  for word in nsi:\n",
        "    try :\n",
        "      if doc[word] > 0:\n",
        "        score[counter] += 1\n",
        "        # you can comment the next line\n",
        "        score[counter] += 0.02 * doc[word]\n",
        "    except :\n",
        "      pass\n",
        "  counter += 1\n",
        "sorted_score = {k: v for k, v in sorted(score.items(), key=lambda item: item[1], reverse=True)}"
      ],
      "metadata": {
        "id": "ys8OmftTt8QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXuPEBPaJgjk",
        "outputId": "76f7f51d-ebe0-434d-89d5-24af5e0899bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "آتروفی یا تحلیل عضلات: علل، علائم، پیشگیری و درمان\n",
            "https://www.hidoctor.ir/328173_%d8%a2%d8%aa%d8%b1%d9%88%d9%81%db%8c-%db%8c%d8%a7-%d8%aa%d8%ad%d9%84%db%8c%d9%84-%d8%b9%d8%b6%d9%84%d8%a7%d8%aa.html/\n",
            "\n",
            "اطلاعات جامع در مورد بیماری منییر\n",
            "https://www.hidoctor.ir/276282_%d8%a7%d8%b7%d9%84%d8%a7%d8%b9%d8%a7%d8%aa-%d8%ac%d8%a7%d9%85%d8%b9-%d8%af%d8%b1-%d9%85%d9%88%d8%b1%d8%af-%d8%a8%db%8c%d9%85%d8%a7%d8%b1%db%8c-%d9%85%d9%86%db%8c%db%8c%d8%b1.html/\n",
            "\n",
            "هورمون رشد خوبه یا بد؟\n",
            "https://namnak.com/هورمون-رشد.p33051\n",
            "\n",
            "اطلاعاتی در مورد سرطان و دیابت\n",
            "https://www.hidoctor.ir/275852_%d8%a7%d8%b7%d9%84%d8%a7%d8%b9%d8%a7%d8%aa%db%8c-%d8%af%d8%b1-%d9%85%d9%88%d8%b1%d8%af-%d8%b3%d8%b1%d8%b7%d8%a7%d9%86-%d9%88-%d8%af%db%8c%d8%a7%d8%a8%d8%aa.html/\n",
            "\n",
            "سرمه کشیدن و این همه خاصیت برای سلامت چشم\n",
            "https://www.hidoctor.ir/285564_%d8%b3%d8%b1%d9%85%d9%87-%da%a9%d8%b4%db%8c%d8%af%d9%86-%d9%88-%d8%a7%db%8c%d9%86-%d9%87%d9%85%d9%87-%d8%ae%d8%a7%d8%b5%db%8c%d8%aa-%d8%a8%d8%b1%d8%a7%db%8c-%d8%b3%d9%84%d8%a7%d9%85%d8%aa-%da%86%d8%b4.html/\n",
            "\n",
            "جراحی ترمیمی مجاری ادراری + راهنمایی کامل\n",
            "https://www.hidoctor.ir/345053_%d9%85%d8%ac%d8%a7%d8%b1%db%8c-%d8%a7%d8%af%d8%b1%d8%a7%d8%b1%db%8c.html/\n",
            "\n",
            "نحوه مصرف ویتامین E\n",
            "https://www.hidoctor.ir/256262_%d9%86%d8%ad%d9%88%d9%87-%d9%85%d8%b5%d8%b1%d9%81-%d9%88%db%8c%d8%aa%d8%a7%d9%85%db%8c%d9%86-e.html/\n",
            "\n",
            "عرق سرد و این خطرات\n",
            "https://www.hidoctor.ir/135026_%d8%b9%d8%b1%d9%82-%d8%b3%d8%b1%d8%af-%d9%88-%d8%a7%db%8c%d9%86-%d8%ae%d8%b7%d8%b1%d8%a7%d8%aa.html/\n",
            "\n",
            "شناسایی سلامتی بدن با عملکرد چشم\n",
            "https://namnak.com/عملکرد-چشم.p43729\n",
            "\n",
            "گرفتگی عضلات نشان دهنده چه مشکلاتی است؟ + علت\n",
            "https://namnak.com/گرفتگی-عضلات.p42123\n",
            "\n",
            "فلج مغزی چیست؟ علائم، علل، درمان بیماری CP\n",
            "https://www.hidoctor.ir/337767_%d9%81%d9%84%d8%ac-%d9%85%d8%ba%d8%b2%db%8c.html/\n",
            "\n",
            "فتوفوبیا یا حساسیت چشم به نور وقتی جدی می شود\n",
            "https://namnak.com/فتوفوبیا.p2610\n",
            "\n"
          ]
        }
      ],
      "source": [
        "counter = 0\n",
        "for x in sorted_score:\n",
        "  if counter >= k:\n",
        "    break\n",
        "  print(bioset[x]['title'])\n",
        "  print(bioset[x]['link'])\n",
        "  print()\n",
        "  counter += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PODaDRrrLGkE"
      },
      "source": [
        "**Alternative solution**\n",
        "\n",
        "we can do all these things and findout the scores without num_of_words.\n",
        "here is alternative solution for this problem:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVlObgPZnqKE"
      },
      "outputs": [],
      "source": [
        "# input = input(\"Enter your subjects :\")\n",
        "input = 'ارتباط حمله قلبی و کرونا'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frSnBKUGAx8I"
      },
      "outputs": [],
      "source": [
        "score = []\n",
        "def reset_score():\n",
        "  for i in range(len(all_tokens_nonstop_lemstem)):\n",
        "    score.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEcOhn9yA0LO"
      },
      "outputs": [],
      "source": [
        "splitted_input = input.split(\" \")\n",
        "nsi = []\n",
        "for x in splitted_input:\n",
        "  if x not in stopwords:\n",
        "    nsi.append(normalizer.normalize(lemmatizer.lemmatize(x))) \n",
        "reset_score()\n",
        "counter = 0\n",
        "for doc in all_tokens_nonstop_lemstem :\n",
        "  for x in nsi:\n",
        "    if x in doc:\n",
        "      score[counter]+=1\n",
        "  counter += 1\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-liM9gvK8pd"
      },
      "source": [
        "# <strong><font color=#5F9EA0></span> tf−idf </strong> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMyuZouj3D89"
      },
      "source": [
        "Download all the requirements especially TfidfVectorizer from sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfmrYNpbd96L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "1w5tTod5nI24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ebb6a1a-b6d2-49a1-caf4-9076320956fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use TfidfVectorizer with (1, 2) ngram and norm l2 for our vectorizer.\n",
        "Then we find all the documents' vectors with fit_transform."
      ],
      "metadata": {
        "id": "eYJHapkznect"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNpPBXvS200s"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(use_idf = True, norm ='l2', ngram_range=(1,2), analyzer='word')\n",
        "doc_term_mat = vectorizer.fit_transform([' '.join(doc) for doc in all_tokens_nonstop_lemstem])\n",
        "vocabulary = vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this function we enter the query and do all the preprocessing on that."
      ],
      "metadata": {
        "id": "3ynWJASGfl20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_query(query):\n",
        "  splitted_input = query.split(\" \")\n",
        "  nsi = []\n",
        "  for x in splitted_input:\n",
        "    if x not in stopwords:\n",
        "      nsi.append([normalizer.normalize(lemmatizer.lemmatize(x))])\n",
        "  return nsi"
      ],
      "metadata": {
        "id": "J34mXjV0ZuTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare the query with our documents we need to find query_vector."
      ],
      "metadata": {
        "id": "bkl2eLi-fwLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_query_vector(tokens):\n",
        "  vector = np.zeros(len(vocabulary))\n",
        "  for token in itertools.chain(*tokens):\n",
        "      try:\n",
        "          index = vectorizer.vocabulary_[token]\n",
        "          vector[index] = 1\n",
        "      except ValueError:\n",
        "          pass\n",
        "  return vector"
      ],
      "metadata": {
        "id": "-PjXY3pWZJEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After calculating query_vector, we need To see how close one doc is to the query. So we find the cosine distant between query_vector and each doc. Then we sort the indexes according to the scores."
      ],
      "metadata": {
        "id": "D8Ftuyepf5m6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query, k):\n",
        "  scores = []\n",
        "  tokens = process_query(query)\n",
        "  query_vector = find_query_vector(tokens)\n",
        "\n",
        "  for doc in doc_term_mat.A:\n",
        "    scores.append(np.dot(query_vector, doc)/(np.linalg.norm(query_vector)*np.linalg.norm(doc)))\n",
        "  return np.array(scores).argsort()[-k:][::-1]"
      ],
      "metadata": {
        "id": "SJF69CQWZ2Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"نشانه های سکته مغزی\"\n",
        "x = search(query, k = 12)\n",
        "for index in x:\n",
        "  print(bioset[index]['title'])\n",
        "  print(bioset[index]['link'])\n",
        "  print()"
      ],
      "metadata": {
        "id": "Vo37LTzlahct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRupn88kLDry"
      },
      "source": [
        "# <strong><font color=#5F9EA0></span> Transformer </strong> "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we use trasformer models to embe"
      ],
      "metadata": {
        "id": "2_um3zTnLjgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers faiss torch"
      ],
      "metadata": {
        "id": "GVdrYot_md0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idzLBNvALGT-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, AutoModel, TFAutoModel, BigBirdModel\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if gpu is available or not:"
      ],
      "metadata": {
        "id": "rO9j-x0GOac5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "U1M-X_Ove0Wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are not many Persian models for Transformers. \n",
        "\n",
        "For this reason, we use one of the two models, **Parsbert** or **ParsBigBird**."
      ],
      "metadata": {
        "id": "3DbNe3GcPBx4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzi0Aq6hPBca"
      },
      "outputs": [],
      "source": [
        "choice = 2\n",
        "\n",
        "if choice == 1:\n",
        "  model_name_or_path = \"HooshvareLab/bert-fa-zwnj-base\"\n",
        "  config = AutoConfig.from_pretrained(model_name_or_path)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "  # model = TFAutoModel.from_pretrained(model_name_or_path).to(device)\n",
        "  model = AutoModel.from_pretrained(model_name_or_path).to(device)\n",
        "\n",
        "if choice == 2:\n",
        "\n",
        "  MODEL_NAME = \"SajjadAyoubi/distil-bigbird-fa-zwnj\"\n",
        "\n",
        "  # model = BigBirdModel.from_pretrained(MODEL_NAME, block_size=32).to(device)\n",
        "  model = BigBirdModel.from_pretrained(MODEL_NAME, attention_type=\"original_full\").to(device)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print(model.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use preprocessed data and join them together for further works:"
      ],
      "metadata": {
        "id": "WK0wYT_MXjeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_documents = [' '.join(x) for x in all_tokens_nonstop_lemstem]"
      ],
      "metadata": {
        "id": "C1hfZLThQm5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunalely available models for transformers can not process documents with more than 512 characters. so we partition each document to sections with 512 or less characters. Afterwards, If our data length exceeds the specified limit, partition_doc function find their embeddings and calculate the average of the vectors as the final embeddeding for each document recursively. Otherwise we calculate the embedding simply by the model."
      ],
      "metadata": {
        "id": "0fzOcXkWX2Ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def partition_doc(document):\n",
        "  x = []\n",
        "  def partition(document):\n",
        "    if len(document) == 0:\n",
        "      return x\n",
        "    t = tokenizer(document[:512], return_tensors='pt').to(device)\n",
        "    m = model(**t)[0].detach().squeeze()\n",
        "    x.append(torch.mean(m, dim=0))\n",
        "    return partition(document[512:])\n",
        "  z = partition(document)\n",
        "  return z"
      ],
      "metadata": {
        "id": "lDFLedX4M3lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this loop we tokenize the document and return it as PyTorch tensors and pass it onto the model:\n"
      ],
      "metadata": {
        "id": "SV7oE3B9ghME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "averaged_vectors = []\n",
        "counter = 0 \n",
        "for document in transformer_documents:\n",
        "  try:\n",
        "    x = tokenizer(document, return_tensors='pt').to(device)\n",
        "    m = model(**x)[0].detach().squeeze()\n",
        "    averaged_vectors.append(torch.mean(m, dim=0))\n",
        "  except:\n",
        "    tensors = partition_doc(document)\n",
        "    result = torch.stack(tensors, dim=0).sum(dim=0)\n",
        "    averaged_vectors.append(torch.div(result, len(tensors)))\n",
        "  if counter % 100 == 0:\n",
        "    print(counter)\n",
        "  counter +=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H67w6hshoD0y",
        "outputId": "6304e9e7-5871-4b38-87ec-c76528d4300a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1393 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[v.size() for v in averaged_vectors[:5]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZfrwyQejUDY",
        "outputId": "2738ea17-f2cf-4397-ae7d-038b33da0e15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[torch.Size([768]),\n",
              " torch.Size([768]),\n",
              " torch.Size([768]),\n",
              " torch.Size([768]),\n",
              " torch.Size([768])]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = tokenizer(transformer_documents[1], return_tensors='pt').to(device)\n",
        "m = model(**x)[0].detach().squeeze()\n",
        "m"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbcGRMnTk7Ll",
        "outputId": "2f412bb0-c32d-4d0c-fed2-8930a12a4584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.7518, -0.2972,  0.5164,  ...,  0.9054, -1.0787, -0.7217],\n",
              "        [ 0.9634, -0.1464, -0.5599,  ...,  0.0650,  1.0207,  0.7631],\n",
              "        [-0.7309,  0.3546, -0.2604,  ...,  0.0146,  1.0128, -0.4903],\n",
              "        ...,\n",
              "        [ 0.4148, -0.3143, -0.1375,  ...,  0.5281,  0.2779, -1.3112],\n",
              "        [ 0.7779, -1.2898,  0.0992,  ...,  0.3979,  0.1694, -1.8513],\n",
              "        [ 1.7691, -0.2985,  0.4894,  ...,  0.8071,  0.4125,  0.0774]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install the requirements for using faiss and query and searching!"
      ],
      "metadata": {
        "id": "a0c-kC4hg5NR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install libomp-dev"
      ],
      "metadata": {
        "id": "ZzgwIjpPcPzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "n56GdjbmBudm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Index all the documents, we need them as numpy arrays first\n"
      ],
      "metadata": {
        "id": "TW9d6xaWhft7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = faiss.IndexIDMap(faiss.IndexFlatIP(768))\n",
        "index.add_with_ids(np.array([t.cpu().numpy() for t in averaged_vectors]), np.array(range(0, len(documents))))"
      ],
      "metadata": {
        "id": "7xDVCixNFuUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With *search* and *encoder* function we can encode queries into the vector space and find the most relevant documents to them with the help of faiss. Also we calculate the score of each document and return them with respect to ***k***."
      ],
      "metadata": {
        "id": "SCL6x7WKh9TG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder(document: str) -> torch.Tensor:\n",
        "  tokens = tokenizer(document, return_tensors='pt').to(device)\n",
        "  vector = model(**tokens)[0].detach().squeeze()\n",
        "  return torch.mean(vector, dim=0)"
      ],
      "metadata": {
        "id": "y2QdDxKWlZpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query: str, k=12):\n",
        "  encoded_query = encoder(query).unsqueeze(dim=0).cpu().numpy()\n",
        "  top_k = index.search(encoded_query, k)\n",
        "  scores = top_k[0][0]\n",
        "  return top_k[1][0]"
      ],
      "metadata": {
        "id": "c12hHQ9OhrAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we can enter our input and after normalization and lemmatization of the query, we return top ***k*** relevant documents titles and links."
      ],
      "metadata": {
        "id": "bmnl8WEzkIGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = 'نقش استرس در حمله قلبی'\n",
        "splitted_input = input.split(\" \")\n",
        "nsi = []\n",
        "k = 12\n",
        "for x in splitted_input:\n",
        "  if x not in stopwords:\n",
        "    nsi.append(normalizer.normalize(lemmatizer.lemmatize(x))) \n",
        "\n",
        "input = \" \".join(nsi)"
      ],
      "metadata": {
        "id": "Pz3e6C12R--Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = search(input, k)\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk6Id7MsgSIU",
        "outputId": "0abd4847-d3ca-4197-e568-feeae8d1a31e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 438, 1017, 1169, 1917,  184,  932, 1370, 1427, 1619, 1014,  267,\n",
              "       1516])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in res:\n",
        "  print(bioset[x]['title'])\n",
        "  print(bioset[x]['link'])\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKK8f-nIdjaL",
        "outputId": "8ca64be6-95a8-43ca-f83a-d01df5ba8801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ناشتای سالم برای افراد سالم\n",
            "https://www.hidoctor.ir/36118_%d9%86%d8%a7%d8%b4%d8%aa%d8%a7%db%8c-%d8%b3%d8%a7%d9%84%d9%85-%d8%a8%d8%b1%d8%a7%db%8c-%d8%a7%d9%81%d8%b1%d8%a7%d8%af-%d8%b3%d8%a7%d9%84%d9%85.html/\n",
            "\n",
            "سودمندی ویتامین دی در بهبود كمردرد\n",
            "https://www.hidoctor.ir/1756_%d8%b3%d9%88%d8%af%d9%85%d9%86%d8%af%db%8c-%d9%88%db%8c%d8%aa%d8%a7%d9%85%db%8c%d9%86-%d8%af%db%8c-%d8%af%d8%b1-%d8%a8%d9%87%d8%a8%d9%88%d8%af-%d9%83%d9%85%d8%b1%d8%af%d8%b1%d8%af.html/\n",
            "\n",
            "غذا خوردن به همراه دوستان و آشنایان\n",
            "https://www.hidoctor.ir/137551_%d8%ba%d8%b0%d8%a7-%d8%ae%d9%88%d8%b1%d8%af%d9%86-%d8%a8%d9%87-%d9%87%d9%85%d8%b1%d8%a7%d9%87-%d8%af%d9%88%d8%b3%d8%aa%d8%a7%d9%86-%d9%88-%d8%a2%d8%b4%d9%86%d8%a7%db%8c%d8%a7%d9%86.html/\n",
            "\n",
            "ورزش کردن راهکاری موثر برای پیشگیری از آلزایمر\n",
            "https://namnak.com/پیشگیری-از-آلزایمر.p49776\n",
            "\n",
            "ارتباط بین تغذیه و سلامتی در زندگی انسان\n",
            "https://www.hidoctor.ir/125875_%d8%a7%d8%b1%d8%aa%d8%a8%d8%a7%d8%b7-%d8%a8%db%8c%d9%86-%d8%aa%d8%ba%d8%b0%db%8c%d9%87-%d9%88-%d8%b3%d9%84%d8%a7%d9%85%d8%aa%db%8c-%d8%af%d8%b1-%d8%b2%d9%86%d8%af%da%af%db%8c-%d8%a7%d9%86%d8%b3%d8%a7.html/\n",
            "\n",
            "ارتباط بین تغذیه و سلامتی در زندگی انسان\n",
            "https://www.hidoctor.ir/125875_%d8%a7%d8%b1%d8%aa%d8%a8%d8%a7%d8%b7-%d8%a8%db%8c%d9%86-%d8%aa%d8%ba%d8%b0%db%8c%d9%87-%d9%88-%d8%b3%d9%84%d8%a7%d9%85%d8%aa%db%8c-%d8%af%d8%b1-%d8%b2%d9%86%d8%af%da%af%db%8c-%d8%a7%d9%86%d8%b3%d8%a7.html/\n",
            "\n",
            "اثرات مخرب دیابت بر بافت دندان ها\n",
            "https://namnak.com/بافت-دندان.p53130\n",
            "\n",
            "نقش مهم روابط خانوادگی در سلامت روان\n",
            "https://namnak.com/نقش-روابط-خانوادگی-در-سلامت-روان.p24526\n",
            "\n",
            "علت افسردگی این بار در اتاق خواب شماست\n",
            "https://namnak.com/علت-افسردگی.p65729\n",
            "\n",
            "واکسن سرطان آنتي ژن به کمک سرطان کليه مي آيد\n",
            "https://www.hidoctor.ir/685_%d9%88%d8%a7%da%a9%d8%b3%d9%86-%d8%b3%d8%b1%d8%b7%d8%a7%d9%86-%d8%a2%d9%86%d8%aa%d9%8a-%da%98%d9%86-%d8%a8%d9%87-%da%a9%d9%85%da%a9-%d8%b3%d8%b1%d8%b7%d8%a7%d9%86-%da%a9%d9%84%d9%8a%d9%87-%d9%85%d9%8a.html/\n",
            "\n",
            "مصرف دارو در دوران شيردهي مادر\n",
            "https://www.hidoctor.ir/541_%d9%85%d8%b5%d8%b1%d9%81-%d8%af%d8%a7%d8%b1%d9%88-%d8%af%d8%b1-%d8%af%d9%88%d8%b1%d8%a7%d9%86-%d8%b4%d9%8a%d8%b1%d8%af%d9%87%d9%8a-%d9%85%d8%a7%d8%af%d8%b1.html/\n",
            "\n",
            "خوراکی های که باعث کوتاهی عمر می شوند\n",
            "https://namnak.com/کوتاهی-عمر.p52572\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMtSx2HVLG_K"
      },
      "source": [
        "# <strong><font color=#5F9EA0></span> fasttext </strong> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFJuRc0vTKUg"
      },
      "source": [
        "ّFirst we need to download, extract and load Fasttext word embedding model to use it in our code: \n",
        "\n",
        "*   Trained persian language dataset\n",
        "*   fasttext library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJAv-AXNLSqT"
      },
      "outputs": [],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QM2B_B3SUbDW"
      },
      "outputs": [],
      "source": [
        "# !gunzip ../content/cc.fa.300.bin.gz\n",
        "!pip install fasttext\n",
        "# import fasttext \n",
        "\n",
        "# model = fasttext.load_model(\"/content/cc.fa.300.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rfr8XptNV1GA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dec8f182-5422-458b-91c9-7321b4492d1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n"
          ]
        }
      ],
      "source": [
        "import fasttext.util\n",
        "fasttext.util.download_model('fa', if_exists='ignore')  # Persian\n",
        "ft = fasttext.load_model('cc.fa.300.bin')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig0L3B0gTkaT"
      },
      "source": [
        "We need numpy to calculate vector calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNPGjwDQd2JQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzrJG4AVjAKR"
      },
      "source": [
        "Now you need to Enter a Query that we with Fasttext method later find the best results. This query first will be pre-processed and then the vectors based on our trained dataset will be built."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_Gr6IeWTk6o"
      },
      "outputs": [],
      "source": [
        "print(\"Search: \", end='')\n",
        "search = input()\n",
        "search_words_splited = search.split(\" \")\n",
        "vectors = []\n",
        "\n",
        "search_words = []\n",
        "for x in search_words_splited:\n",
        "  if x not in stopwords:\n",
        "    search_words.append([normalizer.normalize(lemmatizer.lemmatize(x))])\n",
        "\n",
        "for word in search_words:\n",
        "  vectors.append(np.array(ft.get_word_vector(word)))\n",
        "\n",
        "vectors = np.array(vectors)\n",
        "search_vector = np.mean(vectors, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quBf3Fjvi2kC"
      },
      "source": [
        "To compare the query with our documents we will also need these docs' vectors.<br>\n",
        "So in this section all of the doc's vectors will be build."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdGRWVlzf66E"
      },
      "outputs": [],
      "source": [
        "doc_vectors = []\n",
        "for i in range(len(all_tokens_nonstop)):\n",
        "  vectors = []\n",
        "  for word in all_tokens_nonstop[i]:\n",
        "    vectors.append(np.array(ft.get_word_vector(word)))\n",
        "  vectors = np.array(vectors)\n",
        "  doc_vectors.append(np.mean(vectors, axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "602n9IVBjIWi"
      },
      "source": [
        "To see how close one doc is to the query we Find the cosine distant between Query-vector and each Doc-vectors.\n",
        "<br>\n",
        "<strong><font color=#cf6679>Also</span> \"k\" is set to be 10 you can change it yourself</strong> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NCE4DPukhTQ"
      },
      "outputs": [],
      "source": [
        "k = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Skq8ueOYjJZN"
      },
      "outputs": [],
      "source": [
        "distance_list = []\n",
        "\n",
        "for doc_vector in doc_vectors:\n",
        "  distance_list.append(np.dot(search_vector, doc_vector)/(np.linalg.norm(search_vector)*np.linalg.norm(doc_vector)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T2q-3oklmf8"
      },
      "source": [
        "Now we sort the distant list and find the best matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbwEjYvHlmyL"
      },
      "outputs": [],
      "source": [
        "sorted_dist_list = []\n",
        "for doc_vec in distance_list:\n",
        "  sorted_dist_list.append(doc_vec)\n",
        "sorted_dist_list.sort(reverse=True)\n",
        "doc_indices = []\n",
        "for i in range(k):\n",
        "  doc_indices.append(distance_list.index(sorted_dist_list[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing the information of best matches."
      ],
      "metadata": {
        "id": "BYfdEcCJf6oJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQSyV_8jndqn"
      },
      "outputs": [],
      "source": [
        "print(doc_indices)\n",
        "for i in range(k):\n",
        "  print(bioset[doc_indices[i]]['title'])\n",
        "  print(bioset[doc_indices[i]]['link'])\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <strong><font color=#5F9EA0></span> Comparison between methods </strong> "
      ],
      "metadata": {
        "id": "Q2AdjFRM0Lzl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**According to the MRR rank:**"
      ],
      "metadata": {
        "id": "s77GcIax2GG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best method is **tf-idf**. It has performed better than other methods in terms of both mean scores and opinions. The **transformer** method was expected to be better than the other methods, but due to the fact that the processed language is Persian, the existing models for Persian transformers perform much weaker than English, and there were limitations in using them for this volume. Also, in general, the **fasttext** method works better than the regular **boolean** method, but here, by modifying the **boolean** method in our code and paying attention to the number of words in addition to their presence or absence, we made these two methods work almost identically, which shows the effect of our work and the changes which we applied to the **boolean** method.\n",
        "\n",
        "So in conclusion in our project the following relationship is established in terms of the efficiency of the methods:\n",
        "\n",
        "**tf-idf** > **modified boolean** = **fasttext** > **boolean** > **transformer**"
      ],
      "metadata": {
        "id": "eTHeI7ee2WNX"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Dul1eni14UhG",
        "kZWE3sU4JLjY",
        "lVlyHY3kKrhK",
        "0-liM9gvK8pd",
        "DRupn88kLDry",
        "bMtSx2HVLG_K",
        "Q2AdjFRM0Lzl"
      ],
      "name": "MIR_HW2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}